{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team member 4 Review - Shruthi \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarity and readability of my code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I structured my script to be clean, readable, and easy to follow. I used clear, descriptive variable names and broke down the logic into well separated stages. The program first reads the collated_answers.txt file, then processes the answer sequences, and finally produces two plots. I also left commented out import lines from the other team members’ scripts to indicate that I had initially tried integrating their work (as per my role's requirement) before encountering issues that required me to write my own functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness of functionality\n",
    "\n",
    "My script works as expected. It reads the collated file, calculates the mean answers per question, and displays both a scatter plot and a line plot. I used * as the separator between each respondent’s answers and excluded zeros from the mean calculation as instructed. I ran several test cases and ensured that all plots were generated correctly and without crashing. The core functionality is complete and dependable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of docustrings and comments\n",
    "\n",
    "I wrote full and helpful docstrings for each function. I explained the purpose, the parameters, the return values, and any notes. These docstrings show that I understand how the functions work and help make the script easy for other teammates or markers to understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding style and consistency\n",
    "\n",
    "I used a consistent style throughout the script. The naming conventions, spacing, indentation, and structure are all tidy and professional. I included a main function and wrapped it in if name equals main so the script only runs when executed directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bugs and handling\n",
    "\n",
    "One of the biggest bugs I encountered was caused by non numeric lines in the file, like question headers or extra formatting. These lines caused the script to crash when trying to convert them to integers. I fixed this by checking that each line was a digit before processing it. I also handled missing answers by skipping over zeros in the averaging step, which avoids skewed results and makes the script more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code reusability and modularity\n",
    "\n",
    "I broke the script down into two main reusable functions generate_means_sequence and visualize_data. The visualisation function is flexible and can produce either a scatter plot or a line plot depending on the argument passed. These can be reused in future analysis or testing. Everything is modular and easy to understand alone too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependecies and file handling\n",
    "\n",
    "I used necessary libraries like os, numpy, and matplotlib. I dynamically built file paths with os.path.join to keep things portable across systems. I also assumed the collated_answers.txt file already existed in the output folder so that the script could run straight away without any extra setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some issues with integration\n",
    "\n",
    "I encountered multiple integration problems while trying to work with code from other team members.\n",
    "\n",
    "The extract_answers_sequence function in data_extraction_M1.py attempted to use the lower method on a list, which caused it to crash. Because of this bug, I chose not to use this function and wrote my own logic for parsing the collated file.\n",
    "\n",
    "The collate_answer_files function from data_preparation_M2.py used requests.get to handle remote downloads, which is consistent with the brief. However, since I was working with a locally extracted folder rather than actual cloud links, I decided not to run that function and instead assumed collated_answers.txt was already in place.\n",
    "\n",
    "The generate_means_sequence function in data_analysis_M3.py expected a folder of individual files rather than the merged collated_answers.txt file. When I tried to use it, it threw a NotADirectoryError. The visualize_data function also assumed the wrong input format and relied on that same folder logic, which made it unusable in the context of the final integration script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions I took\n",
    "\n",
    "To make sure everything worked smoothly, I rewrote my own versions of generate_means_sequence and visualize_data. I designed these functions to accept collated_answers.txt directly as input and work with its expected format. This made the script robust and aligned with the final data flow.\n",
    "\n",
    "I also cleaned the collated_answers.txt file containing just numerical answers, separated by * between each respondent. This allowed me to test the script and prove that both the calculations and the visualisations work properly. When I needed to demonstrate the full pipeline, I used this mock data as proof of concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the Output Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "\n",
    "The scatter plot my program generated shows the average answer chosen per question across all respondents. In the current test data, only the first few questions had answer values, which is why you see a few points at the beginning of the graph and everything else flat at zero. This behaviour confirms that the mean calculation is working properly and that zeros are being inserted when no data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line plot \n",
    "\n",
    "The line plot visualises how each respondent answered each question. For this test, three clear answer patterns are shown. One increases steadily from 1 to 4, another decreases from 4 to 1, and the third has a middle-peaked zig-zag shape. This proves that the data is being parsed and displayed correctly per respondent and that each answer sequence is isolated as intended. With full response data, this graph could be helpful in identifying answer underlying patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall reflection\n",
    "\n",
    "I made sure my contribution to the team project was thorough, correct, and clearly documented. I handled the technical issues independently, wrote my own working solutions, and included explanations for everything I did. I ensured the integration worked even though several of the original module functions were not usable. My script performs all required tasks and is capable of producing the expected statistical plots from the dataset. I also used GitHub to track my commits and will be logging and resolving issues to contribute to project management as well."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
